{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules.\n",
    "import os\n",
    "import matplotlib.pyplot as mp\n",
    "\n",
    "# Import tools for keras.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy         import stats\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import copy\n",
    "import json\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'ModelBase' from 'C:\\\\Users\\\\micha\\\\Documents\\\\CS101\\\\seis-GAN\\\\ModelBase.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imp import reload\n",
    "import TensorFlowUtils\n",
    "reload(TensorFlowUtils)\n",
    "import SeisUtils\n",
    "reload(SeisUtils)\n",
    "import GAN\n",
    "reload(GAN)\n",
    "import ModelBase\n",
    "reload(ModelBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TensorFlowUtils import show_graph\n",
    "from SeisUtils       import extract_func, plot_wforms\n",
    "from SeisUtils       import SeisData\n",
    "from SeisUtils       import SeisGenerator\n",
    "from GAN             import GAN\n",
    "from ModelBase       import ModelBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found project directory \"GAN_2019-05-09-B\": True\n",
      "Loaded config \"GAN_2019-05-09-B\\config.json\":\n",
      " {\n",
      "    \"data_path\": \"wformMat_jpm4p_181106_downsample-5x.h5\",\n",
      "    \"data_format\": \"channels_last\",\n",
      "    \"train_frac\": 0.74,\n",
      "    \"burn_seconds\": 2.5,\n",
      "    \"input_seconds\": 80,\n",
      "    \"output_seconds\": null,\n",
      "    \"measure_rate\": 20,\n",
      "    \"normalize_data\": true,\n",
      "    \"predict_normalization_factor\": true,\n",
      "    \"batch_size\": 256,\n",
      "    \"epochs\": 200,\n",
      "    \"random_seed\": 7,\n",
      "    \"load_data_into_memory\": false,\n",
      "    \"metas\": [\n",
      "        \"dist\",\n",
      "        \"magn\"\n",
      "    ],\n",
      "    \"weight_loss\": false,\n",
      "    \"lr_gen\": 0.0001,\n",
      "    \"lr_dis\": 0.0001,\n",
      "    \"conditional_config\": {\n",
      "        \"one_hot_encode\": true,\n",
      "        \"aux_classify\": false\n",
      "    },\n",
      "    \"bins_s\": [\n",
      "        10,\n",
      "        [\n",
      "            3,\n",
      "            4,\n",
      "            4.5,\n",
      "            5,\n",
      "            5.5,\n",
      "            6,\n",
      "            7,\n",
      "            \"inf\"\n",
      "        ]\n",
      "    ],\n",
      "    \"num_channels\": 3,\n",
      "    \"directory\": \"GAN_2019-05-09-B\",\n",
      "    \"restore\": true,\n",
      "    \"restore_ckpt\": null,\n",
      "    \"debug\": true,\n",
      "    \"model_notes\": \"Increase size of fully connected layers.\",\n",
      "    \"normalizers\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "project_dir =  'GAN_2019-05-09-B' # 'GAN_2019-05-05' # 'GAN_2019-04-16-I' # 'GAN_2019-05-09-B' # 'GAN_2019-03-12-B' # 'GAN-2018-02-26-B' 'GAN_2019-03-01' # \n",
    "print('Found project directory \"{}\":'.format(project_dir), True)\n",
    "config_path = os.path.join(project_dir, 'config.json')\n",
    "config_d    = {}\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config_d = json.load(config_file)\n",
    "config_d['restore'] = True\n",
    "config_d['data_path'] = \"wformMat_jpm4p_181106_downsample-5x.h5\"\n",
    "config_d['load_data_into_memory'] = False\n",
    "# config_d['restore_ckpt'] = 'ganckpt.ckpt-121233'\n",
    "# config_d['train_frac'] = 0.74\n",
    "# config_d['transform'] = 'from-' + config_d['transform'] if 'transform' in config_d else False\n",
    "print('Loaded config \"{}\":\\n'.format(config_path), json.dumps(config_d, indent = 4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModel(ModelBase):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super(GANModel, self).__init__(config)\n",
    "        \n",
    "    def build_graph_and_session(self):\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default() as g:\n",
    "            \n",
    "            sessConfig        = tf.ConfigProto(allow_soft_placement = True)\n",
    "            self.sess         = tf.Session(config = sessConfig)\n",
    "            restore_path      = os.path.join(self.checkpoint_dir, self.config.get('restore_ckpt')) if self.config.get('restore_ckpt') else tf.train.latest_checkpoint(self.checkpoint_dir)\n",
    "            self.saver        = tf.train.import_meta_graph(restore_path + '.meta')\n",
    "            self.saver       .restore(self.sess, restore_path)\n",
    "\n",
    "            # Get latent placeholder and output tensor to generator.\n",
    "            self.training_bool_placeholder  = g.get_tensor_by_name('training_bool:0')            \n",
    "            self.latent_placeholder         = g.get_tensor_by_name('latent:0')\n",
    "            self.gen_output_tensor          = g.get_tensor_by_name('gen/output:0')\n",
    "\n",
    "            # Get input placeholder and output tensor to discriminator. \n",
    "            self.dis_input_placeholder = g.get_tensor_by_name('dis/input:0') # Can be used for real or fake.\n",
    "            self.dis_output_tensor     = g.get_tensor_by_name('dis/real/output:0') # @ to change\n",
    "            \n",
    "            # Get global step.\n",
    "            self.global_step = tf.train.get_global_step()\n",
    "                        \n",
    "            # Setup conditional placeholder.\n",
    "            if self.config.get('bins_s'):\n",
    "                self.conditional_placeholders = [g.get_tensor_by_name('conditional:0'), g.get_tensor_by_name('conditional_1:0')]\n",
    "            else:\n",
    "                self.conditional_placeholders = None    \n",
    "            \n",
    "            # Setup predicted normalization tensor.\n",
    "            if self.config.get('predict_normalization_factor'):\n",
    "                self.pred_norm_fake_tensor = g.get_tensor_by_name('gen/pred_norm:0')\n",
    "            else:\n",
    "                self.pred_norm_fake_tensor = None\n",
    "\n",
    "        \n",
    "    #-------------------\n",
    "    # Generator\n",
    "    #-------------------\n",
    "\n",
    "    np.random.seed(7)\n",
    "    def latent(self, N = 4): \n",
    "\n",
    "        latent_shape = [N] + self.latent_placeholder.shape.as_list()[1:]\n",
    "\n",
    "        latent = np.random.normal(size = latent_shape)\n",
    "        return latent\n",
    "\n",
    "    def generator(self, latent, training = False, conditional = None, **kwargs):\n",
    "        \n",
    "        feed_dict = {self.latent_placeholder : latent, self.training_bool_placeholder : training}\n",
    "        if conditional is not None:\n",
    "            feed_dict.update({cp : m for cp, m in zip(self.conditional_placeholders, conditional if isinstance(conditional, list) else [conditional])})\n",
    "            \n",
    "        if self.pred_norm_fake_tensor is not None:\n",
    "            gen, pred_norm = self.sess.run([self.gen_output_tensor, self.pred_norm_fake_tensor], feed_dict = feed_dict)\n",
    "            gen = gen * np.exp(pred_norm[:, None, :])\n",
    "        else:\n",
    "            gen = self.sess.run(self.gen_output_tensor, feed_dict = feed_dict)\n",
    "        \n",
    "        if self.config.get('normalizers'):\n",
    "            gen = self.config['normalizers'][0] * gen\n",
    "            \n",
    "        if self.transformation is not None:\n",
    "            gen = self.transformation.retransform(gen, **kwargs)\n",
    "            \n",
    "        return gen\n",
    "\n",
    "    #-------------------\n",
    "    # Discriminator.\n",
    "    #-------------------\n",
    "\n",
    "    def discriminator(self, input_real_or_fake):\n",
    "\n",
    "        feed_dict = {self.dis_input_placeholder : input_real_or_fake, self.training_bool_placeholder : training}\n",
    "        if conditional is not None:\n",
    "            feed_dict.update({cp : m for cp, m in zip(self.conditional_placeholders, conditional if isinstance(conditional, list) else [conditional])})\n",
    "            \n",
    "        return self.sess.run(self.dis_output_tensor, feed_dict = feed_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config {'data_path': 'wformMat_jpm4p_181106_downsample-5x.h5', 'data_format': 'channels_last', 'train_frac': 0.74, 'burn_seconds': 2.5, 'input_seconds': 80, 'output_seconds': None, 'measure_rate': 20, 'normalize_data': True, 'predict_normalization_factor': True, 'batch_size': 256, 'epochs': 200, 'random_seed': 7, 'load_data_into_memory': False, 'metas': ['dist', 'magn'], 'weight_loss': False, 'lr_gen': 0.0001, 'lr_dis': 0.0001, 'conditional_config': {'one_hot_encode': True, 'aux_classify': False}, 'bins_s': [10, [3, 4, 4.5, 5, 5.5, 6, 7, 'inf']], 'num_channels': 3, 'directory': 'GAN_2019-05-09-B', 'restore': True, 'restore_ckpt': None, 'debug': True, 'model_notes': 'Increase size of fully connected layers.', 'normalizers': None}\n",
      "Testing    Samples: 37666\n",
      "Validation Samples: 30133\n",
      "Training   Samples: 192965 \n",
      "\n",
      "[(260764,), (260764,)]\n",
      "[(11,), (8,)]\n",
      "conditional_metav (shape): [(260764, 10), (260764, 7)]\n",
      "weights: [0.6389864 1.238719  1.238719  ... 2.0003831 2.0003831 2.0003831]\n",
      "   max  =  2137.4094\n",
      "   min  =  0.11315631\n",
      "   mean =  1.0\n",
      "   std  =  8.727127 \n",
      "\n",
      "Testing    Samples: 148\n",
      "Validation Samples: 117\n",
      "Training   Samples: 753\n",
      "WARNING:tensorflow:From c:\\users\\micha\\envs\\seis3.6.7\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from GAN_2019-05-09-B\\./checkpoints\\ganckpt.ckpt-87348\n"
     ]
    }
   ],
   "source": [
    "gm = GANModel(config_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------\n",
    "# Random Batch - for visual comparison\n",
    "#--------------------------------------\n",
    "\n",
    "# Save new example batch if desired.\n",
    "if False:\n",
    "    batch           = gm.SG_train.random_batch(normalize = False)\n",
    "    x               = batch.x\n",
    "    x_normalization = batch.x_normalization\n",
    "    v               = batch.metav # Defaults to None\n",
    "    np.save('example_batch_x.npy', x)\n",
    "    np.save('example_batch_x_normalization.npy', x_normalization)\n",
    "    for i, v_i in enumerate(v):\n",
    "        np.save('example_batch_v_{}.npy'.format(i), v_i)\n",
    "    \n",
    "# Load new example batch.\n",
    "x               = np.load('example_batch_x.npy')\n",
    "x_normalization = np.load('example_batch_x_normalization.npy')\n",
    "v               = []\n",
    "for i in range(2):\n",
    "    v.append(np.load('example_batch_v_{}.npy'.format(i)))\n",
    "\n",
    "#----------------------------\n",
    "# Generated Batch\n",
    "#----------------------------\n",
    "\n",
    "n        = 40 # Number of examples to show\n",
    "N        = gm.batch_size\n",
    "predgen  = gm.generator(gm.latent(N), training = False, conditional = v if v is not None else None)[0:n, ...]\n",
    "m_steps  = gm.config.get('input_seconds', 20) * gm.config.get('measure_rate', 20)\n",
    "realdata = x[0:n, 0:m_steps, :]\n",
    "fig, axs = plot_wforms(\n",
    "    np.vstack([predgen, realdata]),\n",
    "    figsize = (30, np.floor(7.5*n)), \n",
    "    ylabels = ['gen' + str(i+1) for i in range(n)] + ['real' + str(i+1) for i in range(n)],\n",
    "    xlim    = [0, gm.config.get('input_seconds', 20) * gm.config.get('measure_rate', 20)],\n",
    ")\n",
    "\n",
    "#-----------------------------\n",
    "# Plot Examples\n",
    "#-----------------------------\n",
    "    \n",
    "# predgen  = gm.generator(gm.latent(n), conditional = v[0:n, ...])\n",
    "# fig, axs = plot_wforms(predgen, figsize = (30, 30) )\n",
    "for i in range(n):\n",
    "    for j in range(3):\n",
    "        axs[i, j].get_lines()[0].set_c(plt.rcParams['axes.prop_cycle'].by_key()['color'][3])\n",
    "                                   \n",
    "fig.tight_layout() \n",
    "global_step = gm.sess.run(gm.global_step)\n",
    "print('Saving image at global step:', global_step)\n",
    "fig.savefig(os.path.join(project_dir, 'generated_wforms_at_step_{}_{}.png'.format(global_step, str(time.time()).split('.')[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1276-98623c5f19c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;31m# Intentional error to stop 'run all cells'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0 # Intentional error to stop 'run all cells'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
